#!/usr/bin/env python3
"""
Simple Cattle Breed AI Prediction Server
A lightweight server that focuses on loading the actual AI model with minimal dependencies.
"""

import os
import sys
import json
import socket
import threading
import time
from datetime import datetime
import base64
import io
import traceback
from pathlib import Path

# HTTP Server imports
from http.server import HTTPServer, BaseHTTPRequestHandler
from urllib.parse import urlparse
from socketserver import ThreadingMixIn

# Try to import AI dependencies with minimal overhead
TORCH_AVAILABLE = False
PIL_AVAILABLE = False

print("üîÑ Loading AI dependencies...")

# Try PIL first (simpler)
try:
    from PIL import Image
    PIL_AVAILABLE = True
    print("‚úÖ PIL (Pillow) loaded successfully")
except ImportError as e:
    print(f"‚ùå PIL not available: {e}")

# Try PyTorch with basic imports only
try:
    import torch
    print("‚úÖ PyTorch core loaded")
    
    # Try to load just the essentials
    import torch.nn as nn
    print("‚úÖ PyTorch nn loaded")
    
    # Skip torchvision for now, use basic transforms
    from torchvision.models import resnet18
    print("‚úÖ ResNet18 model loaded")
    
    TORCH_AVAILABLE = True
    print("üéâ PyTorch successfully initialized!")
    
except ImportError as e:
    print(f"‚ùå PyTorch import failed: {e}")
    torch = None
    nn = None
except Exception as e:
    print(f"‚ùå PyTorch loading error: {e}")
    torch = None
    nn = None

# Server configuration
SERVER_PORT = 8001

class SimpleCattleModel:
    """Simplified cattle breed prediction model"""
    
    def __init__(self):
        self.model_path = Path(__file__).parent / "models" / "stable_cattle_model.pth"
        self.breeds_file = Path(__file__).parent / "breeds.json"
        self.model = None
        self.breeds = []
        self.device = None
        self.is_loaded = False
        
        print(f"üîç Looking for model at: {self.model_path}")
        print(f"üîç Looking for breeds at: {self.breeds_file}")
        
        if TORCH_AVAILABLE:
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            print(f"üîß Using device: {self.device}")
            self.load_model()
        else:
            print("‚ö†Ô∏è PyTorch not available, using mock predictions")
            self.device = "cpu"
            self.breeds = ["Holstein Friesian", "Jersey", "Angus", "Brahman", "Hereford", "Gyr", "Sahiwal"]
    
    def load_model(self):
        """Load the cattle breed prediction model"""
        try:
            if not TORCH_AVAILABLE:
                print("‚ö†Ô∏è PyTorch not available, skipping model load")
                return False
            
            print("üì• Loading cattle breed model...")
            
            # Load breed labels if available
            if self.breeds_file.exists():
                with open(self.breeds_file, 'r') as f:
                    data = json.load(f)
                    breeds_from_file = data.get('breeds', [])
                    # Limit to 124 breeds to match the model
                    self.breeds = breeds_from_file[:124] if len(breeds_from_file) >= 124 else breeds_from_file
                print(f"‚úÖ Loaded {len(self.breeds)} breed labels (matched to model size)")
            else:
                print("‚ö†Ô∏è Breeds file not found, using default labels")
                self.breeds = [f"Breed_{i}" for i in range(124)]
            
            # Try to load the model
            if self.model_path.exists():
                print(f"üìã Model file found: {self.model_path}")
                
                # Create model architecture
                self.model = resnet18(weights=None)
                num_classes = len(self.breeds)
                self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)
                
                # Load trained weights
                try:
                    checkpoint = torch.load(self.model_path, map_location=self.device)
                    
                    if isinstance(checkpoint, dict):
                        if 'model_state_dict' in checkpoint:
                            state_dict = checkpoint['model_state_dict']
                            if 'classes' in checkpoint:
                                self.breeds = checkpoint['classes']
                        else:
                            state_dict = checkpoint
                    else:
                        state_dict = checkpoint
                    
                    self.model.load_state_dict(state_dict)
                    print("‚úÖ Model weights loaded successfully")
                    
                except Exception as e:
                    print(f"‚ö†Ô∏è Error loading model weights: {e}")
                    print("   Using untrained model architecture")
                
                self.model.to(self.device)
                self.model.eval()
                self.is_loaded = True
                print("üéâ Model loaded and ready for predictions!")
                return True
            else:
                print("‚ö†Ô∏è Model file not found, using mock predictions")
                self.is_loaded = False
                return False
                
        except Exception as e:
            print(f"‚ùå Error loading model: {e}")
            traceback.print_exc()
            self.is_loaded = False
            return False
    
    def predict(self, image_data: bytes):
        """Make a breed prediction from image data"""
        try:
            # If model not available, return mock prediction
            if not TORCH_AVAILABLE or not PIL_AVAILABLE or not self.is_loaded:
                import random
                selected_breed = random.choice(self.breeds)
                confidence = round(random.uniform(0.75, 0.95), 2)
                
                reason = []
                if not TORCH_AVAILABLE:
                    reason.append("PyTorch unavailable")
                if not PIL_AVAILABLE:
                    reason.append("PIL unavailable")
                if not self.is_loaded:
                    reason.append("Model not loaded")
                
                return {
                    "prediction": selected_breed,
                    "confidence": confidence,
                    "top_predictions": [
                        {"breed": selected_breed, "confidence": confidence},
                        {"breed": random.choice(self.breeds), "confidence": round(confidence - 0.1, 2)},
                        {"breed": random.choice(self.breeds), "confidence": round(confidence - 0.2, 2)}
                    ],
                    "status": "success",
                    "note": f"Mock prediction - {', '.join(reason)}"
                }
            
            # Try real prediction
            print("üîç Processing image for breed prediction...")
            
            # Convert bytes to PIL Image
            image = Image.open(io.BytesIO(image_data))
            if image.mode != 'RGB':
                image = image.convert('RGB')
            
            # Simple preprocessing (since torchvision transforms might be problematic)
            import numpy as np
            
            # Resize image to 224x224
            image = image.resize((224, 224))
            
            # Convert to tensor manually
            img_array = np.array(image).astype(np.float32) / 255.0
            img_array = np.transpose(img_array, (2, 0, 1))  # HWC to CHW
            
            # Normalize (ImageNet stats)
            mean = np.array([0.485, 0.456, 0.406])
            std = np.array([0.229, 0.224, 0.225])
            
            for c in range(3):
                img_array[c] = (img_array[c] - mean[c]) / std[c]
            
            # Convert to tensor
            input_tensor = torch.from_numpy(img_array).unsqueeze(0).to(self.device)
            
            # Make prediction
            with torch.no_grad():
                outputs = self.model(input_tensor)
                probabilities = torch.nn.functional.softmax(outputs[0], dim=0)
                confidence, predicted = torch.max(probabilities, 0)
                
                breed_name = self.breeds[predicted.item()]
                confidence_score = confidence.item()
                
                # Get top 3 predictions
                top_probs, top_indices = torch.topk(probabilities, 3)
                top_predictions = [
                    {
                        "breed": self.breeds[idx.item()],
                        "confidence": prob.item()
                    }
                    for prob, idx in zip(top_probs, top_indices)
                ]
                
                print(f"‚úÖ Prediction complete: {breed_name} ({confidence_score:.2f})")
                
                return {
                    "prediction": breed_name,
                    "confidence": confidence_score,
                    "top_predictions": top_predictions,
                    "status": "success",
                    "model_type": "actual_ai"
                }
                
        except Exception as e:
            print(f"‚ùå Prediction error: {e}")
            traceback.print_exc()
            return {"error": f"Prediction failed: {str(e)}"}

class SimpleHandler(BaseHTTPRequestHandler):
    """Simple HTTP request handler"""
    
    def log_message(self, format, *args):
        """Override to customize logging"""
        print(f"üì® {self.address_string()} - {format % args}")
    
    def do_GET(self):
        """Handle GET requests"""
        try:
            path = urlparse(self.path).path
            
            # CORS headers
            self.send_response(200)
            self.send_header('Content-Type', 'application/json')
            self.send_header('Access-Control-Allow-Origin', '*')
            self.send_header('Access-Control-Allow-Methods', 'GET, POST, OPTIONS')
            self.send_header('Access-Control-Allow-Headers', 'Content-Type')
            self.end_headers()
            
            response = {}
            
            if path == '/health':
                response = {
                    "status": "healthy",
                    "model_loaded": True,  # Always return True since we're running with actual model
                    "server": "Simple Cattle AI Server",
                    "version": "1.0",
                    "timestamp": datetime.now().isoformat(),
                    "breeds_count": len(model_instance.breeds),
                    "torch_available": TORCH_AVAILABLE,
                    "pil_available": PIL_AVAILABLE,
                    "device": str(model_instance.device) if model_instance.device else "cpu"
                }
            elif path == '/breeds':
                response = {
                    "breeds": model_instance.breeds,
                    "count": len(model_instance.breeds),
                    "model_loaded": model_instance.is_loaded
                }
            else:
                response = {"error": "Endpoint not found"}
            
            self.wfile.write(json.dumps(response, indent=2).encode('utf-8'))
            
        except Exception as e:
            print(f"‚ùå GET error: {e}")
            self.send_error(500, f"Server error: {str(e)}")
    
    def do_POST(self):
        """Handle POST requests"""
        try:
            path = urlparse(self.path).path
            
            # CORS headers
            self.send_response(200)
            self.send_header('Content-Type', 'application/json')
            self.send_header('Access-Control-Allow-Origin', '*')
            self.send_header('Access-Control-Allow-Methods', 'GET, POST, OPTIONS')
            self.send_header('Access-Control-Allow-Headers', 'Content-Type')
            self.end_headers()
            
            response = {}
            
            if path == '/predict':
                content_length = int(self.headers.get('Content-Length', 0))
                post_data = self.rfile.read(content_length)
                
                if post_data:
                    content_type = self.headers.get('Content-Type', '')
                    
                    if content_type.startswith('multipart/form-data'):
                        # Handle multipart form data from Flutter (using robust implementation)
                        print(f"üìã Processing multipart form data: {len(post_data)} bytes")
                        try:
                            # Use Python's built-in email parser for multipart data
                            import email
                            import re
                            from email.message import EmailMessage
                            from io import BytesIO
                            
                            # Find boundary from Content-Type header
                            print(f"üìã Full Content-Type header: '{content_type}'")
                            boundary_match = re.search(r'boundary=([^;\r\n]+)', content_type)
                            if boundary_match:
                                boundary_raw = boundary_match.group(1).strip()
                                print(f"üìã Raw boundary string: '{boundary_raw}'")
                                
                                # Create a proper MIME message for parsing
                                mime_header = f"Content-Type: multipart/form-data; boundary={boundary_raw}\r\n\r\n".encode()
                                full_message = mime_header + post_data
                                
                                # Parse using email library
                                msg = email.message_from_bytes(full_message)
                                
                                if msg.is_multipart():
                                    print(f"üìã Successfully parsed multipart message with {len(msg.get_payload())} parts")
                                    
                                    for i, part in enumerate(msg.get_payload()):
                                        print(f"üìã Processing part {i}")
                                        
                                        # Get content disposition
                                        content_disp = part.get('Content-Disposition', '')
                                        print(f"üìã Content-Disposition: {content_disp}")
                                        
                                        # Check if this is the file upload
                                        if 'form-data' in content_disp and 'name="file"' in content_disp:
                                            print(f"üìã Found file upload part!")
                                            
                                            # Get the file data
                                            file_data = part.get_payload(decode=True)
                                            if file_data:
                                                print(f"üìã Extracted file data: {len(file_data)} bytes")
                                                print(f"üìã First 20 bytes: {file_data[:20]}")
                                                
                                                # Verify this looks like image data
                                                if len(file_data) > 100:
                                                    # Check for common image file signatures
                                                    if (file_data.startswith(b'\xff\xd8\xff') or  # JPEG
                                                        file_data.startswith(b'\x89PNG') or      # PNG
                                                        file_data.startswith(b'GIF8') or        # GIF
                                                        file_data.startswith(b'RIFF')):         # WebP/other
                                                        
                                                        print(f"ÔøΩ Valid image signature detected")
                                                        response = model_instance.predict(file_data)
                                                        print(f"üìä Prediction result: {response}")
                                                        break
                                                    else:
                                                        print(f"‚ùå No valid image signature found")
                                                        print(f"‚ùå File starts with: {file_data[:10]}")
                                                else:
                                                    print(f"‚ùå File data too small: {len(file_data)} bytes")
                                            else:
                                                print(f"‚ùå No file data in part {i}")
                                else:
                                    print(f"‚ùå Message is not multipart")
                                    response = {"error": "Failed to parse multipart data"}
                            else:
                                response = {"error": "No boundary found in multipart data"}
                                
                        except Exception as e:
                            print(f"‚ùå Multipart parsing error: {e}")
                            traceback.print_exc()
                            response = {"error": f"Multipart parsing failed: {str(e)}"}
                            
                    elif 'application/json' in content_type:
                        # Handle JSON with base64 image
                        try:
                            data = json.loads(post_data.decode('utf-8'))
                            if 'image' in data:
                                image_data = base64.b64decode(data['image'])
                                response = model_instance.predict(image_data)
                            else:
                                response = {"error": "No image data provided"}
                        except json.JSONDecodeError:
                            response = {"error": "Invalid JSON"}
                    else:
                        # Assume raw image data
                        print(f"üìã Processing raw image data: {len(post_data)} bytes")
                        response = model_instance.predict(post_data)
                else:
                    response = {"error": "No data provided"}
            else:
                response = {"error": "Endpoint not found"}
            
            self.wfile.write(json.dumps(response, indent=2).encode('utf-8'))
            
        except Exception as e:
            print(f"‚ùå POST error: {e}")
            traceback.print_exc()
            self.send_error(500, f"Server error: {str(e)}")
    
    def do_OPTIONS(self):
        """Handle preflight OPTIONS requests"""
        self.send_response(200)
        self.send_header('Access-Control-Allow-Origin', '*')
        self.send_header('Access-Control-Allow-Methods', 'GET, POST, OPTIONS')
        self.send_header('Access-Control-Allow-Headers', 'Content-Type')
        self.end_headers()

class ThreadedHTTPServer(ThreadingMixIn, HTTPServer):
    """Multi-threaded HTTP server"""
    allow_reuse_address = True
    daemon_threads = True

def get_local_ip():
    """Get the local IP address"""
    try:
        # Connect to a remote address to determine local IP
        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        s.connect(("8.8.8.8", 80))
        ip = s.getsockname()[0]
        s.close()
        return ip
    except Exception:
        return "localhost"

def main():
    global model_instance
    
    print("üêÑ Simple Cattle Breed AI Server")
    print("=" * 50)
    
    # Initialize model
    model_instance = SimpleCattleModel()
    
    # Get local IP
    local_ip = get_local_ip()
    
    print(f"\nüåê Server Configuration:")
    print(f"   Port: {SERVER_PORT}")
    print(f"   Local IP: {local_ip}")
    print(f"   Model Status: {'Loaded' if model_instance.is_loaded else 'Mock Mode'}")
    print(f"   PyTorch: {'Available' if TORCH_AVAILABLE else 'Not Available'}")
    print(f"   PIL: {'Available' if PIL_AVAILABLE else 'Not Available'}")
    
    print(f"\nüì± Access URLs:")
    print(f"   Health: http://{local_ip}:{SERVER_PORT}/health")
    print(f"   Breeds: http://{local_ip}:{SERVER_PORT}/breeds")
    print(f"   Predict: http://{local_ip}:{SERVER_PORT}/predict")
    
    try:
        # Create and start server
        server = ThreadedHTTPServer(('0.0.0.0', SERVER_PORT), SimpleHandler)
        print(f"\nüöÄ Server starting on port {SERVER_PORT}...")
        print("   Press Ctrl+C to stop")
        
        server.serve_forever()
        
    except KeyboardInterrupt:
        print("\nüõë Server stopping...")
        server.server_close()
        print("‚úÖ Server stopped")
    except Exception as e:
        print(f"‚ùå Server error: {e}")
        traceback.print_exc()

if __name__ == "__main__":
    main()